To
run
one
of
them,
use
`./bin/run-example
<class>
[params]`.
For
example:
./bin/run-example
SparkPi
will
run
the
Pi
example
locally.
You
can
set
the
MASTER
environment
variable
when
running
examples
to
submit
examples
to
a
cluster.
This
can
be
a
mesos://
or
spark://
URL,
"yarn"
to
run
on
YARN,
and
"local"
to
run
locally
with
one
thread,
or
"local[N]"
to
run
locally
with
N
threads.
You
can
also
use
an
abbreviated
class
name
if
the
class
is
in
the
`examples`
package.
For
instance:
MASTER=spark://host:7077
./bin/run-example
SparkPi
Many
of
the
example
programs
print
usage
help
if
no
params
are
given.
##
Running
Tests
Testing
first
requires
[building
Spark](#building-spark).
Once
Spark
is
built,
tests
can
be
run
using:
./dev/run-tests
Please
see
the
guidance
on
how
to
[run
tests
for
a
module,
or
individual
tests](http://spark.apache.org/developer-tools.html#individual-tests).
##
A
Note
About
Hadoop
Versions
Spark
uses
the
Hadoop
core
library
to
talk
to
HDFS
and
other
Hadoop-supported
storage
systems.
Because
the
protocols
have
changed
in
different
versions
of
Hadoop,
you
must
build
Spark
against
the
same
version
that
your
cluster
runs.
Please
refer
to
the
build
documentation
at
["Specifying
the
Hadoop
Version"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)
for
detailed
guidance
on
building
for
a
particular
distribution
of
Hadoop,
including
building
for
particular
Hive
and
Hive
Thriftserver
distributions.
##
Configuration
Please
refer
to
the
[Configuration
Guide](http://spark.apache.org/docs/latest/configuration.html)
in
the
online
documentation
for
an
overview
on
how
to
configure
Spark.
##
Contributing
Please
review
the
[Contribution
to
Spark
guide](http://spark.apache.org/contributing.html)
for
information
on
how
to
get
started
contributing
to
the
project.
